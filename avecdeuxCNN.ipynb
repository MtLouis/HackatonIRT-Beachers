{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "Some utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 - DL tutorial - 1st CNN.ipynb  fileCountLabels.pickle  \u001b[0m\u001b[01;34mhackathon\u001b[0m/\r\n",
      "Db_proportions.h5                finaldB.ipynb           pickle.pickle\r\n",
      "\u001b[01;34mdata\u001b[0m/                            fulltoFFT.ipynb         test.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
      "\u001b[K    100% |################################| 337kB 3.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Collecting pyyaml (from keras)\n",
      "  Downloading PyYAML-3.12.tar.gz (253kB)\n",
      "\u001b[K    100% |################################| 256kB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Running setup.py bdist_wheel for pyyaml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.1.5 pyyaml-3.12\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "PATH_DATA = 'data/full.h5'\n",
    "PATH_PREDICT_WITHOUT_GT = 'data/pred_eighties_from_full_1_without_gt.h5'\n",
    "#PATH_SUBMIT = 'data/submit/pred_eighties_from_half_1_AWESOMEGROUP.h5'\n",
    "#PATH_PREDICT_WITH_GT = 'data/pred_teachers/pred_eighties_from_half_1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout\n",
    "import keras.layers.normalization \n",
    "from keras.callbacks import Callback\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5.File(PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 18698240)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(f['S2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idxs(h5_path):\n",
    "    f = h5.File(h5_path)\n",
    "    return range(len(f['S2']))\n",
    "\n",
    "def shuffle_idx(sample_idxs):\n",
    "    return list(np.random.permutation(sample_idxs))\n",
    "\n",
    "def split_train_val(sample_idxs, proportion):\n",
    "    n_samples = len(sample_idxs)\n",
    "    return sample_idxs[:int((1.-proportion)*n_samples)], sample_idxs[int((1.-proportion)*n_samples):]\n",
    "\n",
    "def get_batch_count(idxs, batch_size):\n",
    "    batch_count = int(len(idxs)//batch_size)\n",
    "    remained_samples = len(idxs)%batch_size\n",
    "    if remained_samples > 0:\n",
    "        batch_count += 1\n",
    "\n",
    "    return batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(h5_path, batch_size, idxs):\n",
    "    f = h5.File(h5_path, 'r')\n",
    "    while True : \n",
    "        idxs = shuffle_idx(idxs)\n",
    "        batch_count = get_batch_count(idxs, batch_size)\n",
    "        for b in range(batch_count):\n",
    "            batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "            batch_idxs = sorted(batch_idxs)\n",
    "            X = f['S2'][batch_idxs, :,:,:]\n",
    "            Y = f['TOP_LANDCOVER'][batch_idxs, :]\n",
    "            yield np.array(X), keras.utils.np_utils.to_categorical(np.array(Y), 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = get_idxs(PATH_DATA)\n",
    "shuffled_idxs = shuffle_idx(idxs)\n",
    "train_idxs, val_idxs = split_train_val(shuffled_idxs, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(PATH_DATA, BATCH_SIZE, train_idxs)\n",
    "train_batch_count = get_batch_count(train_idxs, BATCH_SIZE)\n",
    "\n",
    "val_gen = generator(PATH_DATA, BATCH_SIZE, val_idxs)\n",
    "val_batch_count = get_batch_count(val_idxs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6bd83b136895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TOP_LANDCOVER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcountLabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TOP_LANDCOVER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "countLabels = np.zeros([23,2])\n",
    "\n",
    "for i in range(22):\n",
    "    countLabels[i+1][0] = countLabels[i][0] + 1\n",
    "\n",
    "for i in range(len(f['TOP_LANDCOVER'])):\n",
    "    countLabels[int(f['TOP_LANDCOVER'][i])][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# print(\"countLabels saved\")\n",
    "# fileLabels = open('fileCountLabels.pickle', 'wb')\n",
    "# pickle.dump(countLabels, fileLabels)\n",
    "# fileLabels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 0.000000e+00],\n",
       "       [1.000000e+00, 3.827110e+06],\n",
       "       [2.000000e+00, 1.250253e+06],\n",
       "       [3.000000e+00, 2.315736e+06],\n",
       "       [4.000000e+00, 7.769660e+05],\n",
       "       [5.000000e+00, 2.167443e+06],\n",
       "       [6.000000e+00, 6.330000e+02],\n",
       "       [7.000000e+00, 0.000000e+00],\n",
       "       [8.000000e+00, 0.000000e+00],\n",
       "       [9.000000e+00, 3.709000e+03],\n",
       "       [1.000000e+01, 1.112499e+06],\n",
       "       [1.100000e+01, 7.657050e+05],\n",
       "       [1.200000e+01, 4.054392e+06],\n",
       "       [1.300000e+01, 1.281000e+03],\n",
       "       [1.400000e+01, 6.341420e+05],\n",
       "       [1.500000e+01, 3.847000e+03],\n",
       "       [1.600000e+01, 0.000000e+00],\n",
       "       [1.700000e+01, 8.647100e+04],\n",
       "       [1.800000e+01, 3.829000e+03],\n",
       "       [1.900000e+01, 1.154414e+06],\n",
       "       [2.000000e+01, 5.382200e+05],\n",
       "       [2.100000e+01, 1.590000e+03],\n",
       "       [2.200000e+01, 0.000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from file\n",
    "import pickle\n",
    "fileLabels = open('pickle.pickle', 'rb')\n",
    "countLabels = pickle.load(fileLabels)  # variables come out in the order you put them in\n",
    "fileLabels.close()\n",
    "countLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = np.sum(countLabels[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00],\n",
       "       [1.00000000e+00, 2.04677553e+01],\n",
       "       [2.00000000e+00, 6.68647424e+00],\n",
       "       [3.00000000e+00, 1.23847806e+01],\n",
       "       [4.00000000e+00, 4.15528948e+00],\n",
       "       [5.00000000e+00, 1.15916953e+01],\n",
       "       [6.00000000e+00, 3.38534536e-03],\n",
       "       [7.00000000e+00, 0.00000000e+00],\n",
       "       [8.00000000e+00, 0.00000000e+00],\n",
       "       [9.00000000e+00, 1.98360915e-02],\n",
       "       [1.00000000e+01, 5.94975249e+00],\n",
       "       [1.10000000e+01, 4.09506456e+00],\n",
       "       [1.20000000e+01, 2.16832814e+01],\n",
       "       [1.30000000e+01, 6.85091217e-03],\n",
       "       [1.40000000e+01, 3.39145289e+00],\n",
       "       [1.50000000e+01, 2.05741289e-02],\n",
       "       [1.60000000e+01, 0.00000000e+00],\n",
       "       [1.70000000e+01, 4.62455290e-01],\n",
       "       [1.80000000e+01, 2.04778632e-02],\n",
       "       [1.90000000e+01, 6.17391797e+00],\n",
       "       [2.00000000e+01, 2.87845273e+00],\n",
       "       [2.10000000e+01, 8.50347412e-03],\n",
       "       [2.20000000e+01, 0.00000000e+00]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countlbl = np.zeros([23,2])\n",
    "for i in range(22):\n",
    "    countlbl[i+1][0] = countlbl[i][0] + 1\n",
    "    \n",
    "countlbl[:,1] = 100 * countLabels[:,1]/tot #np.around(100 * countLabels[:,1]/tot, decimals = 3)\n",
    "\n",
    "countlbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 1,  2],\n",
       "       [ 2,  3],\n",
       "       [ 3,  4],\n",
       "       [ 4,  5],\n",
       "       [ 5, 10],\n",
       "       [ 6, 11],\n",
       "       [ 7, 12],\n",
       "       [ 8, 14],\n",
       "       [ 9, 17],\n",
       "       [10, 19],\n",
       "       [11, 20]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tailleDB = 2000000\n",
    "DB = np.zeros([tailleDB, 16, 16, 4, 1])\n",
    "\n",
    "# Correspondance : chaque élément est de type [i,j] où j est la classe originale reliée désormais à l'indice i \n",
    "cor = np.zeros([12,2], dtype=int)\n",
    "for i in range(11):\n",
    "    cor[i+1][0] = cor[i][0] + 1\n",
    "\n",
    "tempLab = 0\n",
    "for i in range(23):\n",
    "    if countlbl[i,1] > 0.2:\n",
    "        cor[tempLab][1] = i\n",
    "        tempLab += 1\n",
    "\n",
    "lblAdded = np.zeros([23,2], dtype = int)\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(f['TOP_LANDCOVER'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998403"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lblComplete = np.zeros([23,2], dtype=int)\n",
    "for i in range(22):\n",
    "    lblComplete[i+1][0] = lblComplete[i][0] + 1\n",
    "\n",
    "for i in range(22):\n",
    "    if i in cor[:,1]:\n",
    "        lblComplete[i,1] = countlbl[i,1]/100 * tailleDB\n",
    "    else:\n",
    "        lblComplete[i,1] = 0\n",
    "    \n",
    "lblComplete\n",
    "np.sum(lblComplete[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff018848470>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff018886550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f['TOP_LANDCOVER'][:2000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBatch = f['S2'][10:10010]\n",
    "dataClass = f['TOP_LANDCOVER'][10:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n",
      "500000\n",
      "501000\n",
      "502000\n",
      "503000\n",
      "504000\n",
      "505000\n",
      "506000\n",
      "507000\n",
      "508000\n",
      "509000\n",
      "510000\n",
      "511000\n",
      "512000\n",
      "513000\n",
      "514000\n",
      "515000\n",
      "516000\n",
      "517000\n",
      "518000\n",
      "519000\n",
      "520000\n",
      "521000\n",
      "522000\n",
      "523000\n",
      "524000\n",
      "525000\n",
      "526000\n",
      "527000\n",
      "528000\n",
      "529000\n",
      "530000\n",
      "531000\n",
      "532000\n",
      "533000\n",
      "534000\n",
      "535000\n",
      "536000\n",
      "537000\n",
      "538000\n",
      "539000\n",
      "540000\n",
      "541000\n",
      "542000\n",
      "543000\n",
      "544000\n",
      "545000\n",
      "546000\n",
      "547000\n",
      "548000\n",
      "549000\n",
      "550000\n",
      "551000\n",
      "552000\n",
      "553000\n",
      "554000\n",
      "555000\n",
      "556000\n",
      "557000\n",
      "558000\n",
      "559000\n",
      "560000\n",
      "561000\n",
      "562000\n",
      "563000\n",
      "564000\n",
      "565000\n",
      "566000\n",
      "567000\n",
      "568000\n",
      "569000\n",
      "570000\n",
      "571000\n",
      "572000\n",
      "573000\n",
      "574000\n",
      "575000\n",
      "576000\n",
      "577000\n",
      "578000\n",
      "579000\n",
      "580000\n",
      "581000\n",
      "582000\n",
      "583000\n",
      "584000\n",
      "585000\n",
      "586000\n",
      "587000\n",
      "588000\n",
      "589000\n",
      "590000\n",
      "591000\n",
      "592000\n",
      "593000\n",
      "594000\n",
      "595000\n",
      "596000\n",
      "597000\n",
      "598000\n",
      "599000\n",
      "600000\n",
      "601000\n",
      "602000\n",
      "603000\n",
      "604000\n",
      "605000\n",
      "606000\n",
      "607000\n",
      "608000\n",
      "609000\n",
      "610000\n",
      "611000\n",
      "612000\n",
      "613000\n",
      "614000\n",
      "615000\n",
      "616000\n",
      "617000\n",
      "618000\n",
      "619000\n",
      "620000\n",
      "621000\n",
      "622000\n",
      "623000\n",
      "624000\n",
      "625000\n",
      "626000\n",
      "627000\n",
      "628000\n",
      "629000\n",
      "630000\n",
      "631000\n",
      "632000\n",
      "633000\n",
      "634000\n",
      "635000\n",
      "636000\n",
      "637000\n",
      "638000\n",
      "639000\n",
      "640000\n",
      "641000\n",
      "642000\n",
      "643000\n",
      "644000\n",
      "645000\n",
      "646000\n",
      "647000\n",
      "648000\n",
      "649000\n",
      "650000\n",
      "651000\n",
      "652000\n",
      "653000\n",
      "654000\n",
      "655000\n",
      "656000\n",
      "657000\n",
      "658000\n",
      "659000\n",
      "660000\n",
      "661000\n",
      "662000\n",
      "663000\n",
      "664000\n",
      "665000\n",
      "666000\n",
      "667000\n",
      "668000\n",
      "669000\n",
      "670000\n",
      "671000\n",
      "672000\n",
      "673000\n",
      "674000\n",
      "675000\n",
      "676000\n",
      "677000\n",
      "678000\n",
      "679000\n",
      "680000\n",
      "681000\n",
      "682000\n",
      "683000\n",
      "684000\n",
      "685000\n",
      "686000\n",
      "687000\n",
      "688000\n",
      "689000\n",
      "690000\n",
      "691000\n",
      "692000\n",
      "693000\n",
      "694000\n",
      "695000\n",
      "696000\n",
      "697000\n",
      "698000\n",
      "699000\n",
      "700000\n",
      "701000\n",
      "702000\n",
      "703000\n",
      "704000\n",
      "705000\n",
      "706000\n",
      "707000\n",
      "708000\n",
      "709000\n",
      "710000\n",
      "711000\n",
      "712000\n",
      "713000\n",
      "714000\n",
      "715000\n",
      "716000\n",
      "717000\n",
      "718000\n",
      "719000\n",
      "720000\n",
      "721000\n",
      "722000\n",
      "723000\n",
      "724000\n",
      "725000\n",
      "726000\n",
      "727000\n",
      "728000\n",
      "729000\n",
      "730000\n",
      "731000\n",
      "732000\n",
      "733000\n",
      "734000\n",
      "735000\n",
      "736000\n",
      "737000\n",
      "738000\n",
      "739000\n",
      "740000\n",
      "741000\n",
      "742000\n",
      "743000\n",
      "744000\n",
      "745000\n",
      "746000\n",
      "747000\n",
      "748000\n",
      "749000\n",
      "750000\n",
      "751000\n",
      "752000\n",
      "753000\n",
      "754000\n",
      "755000\n",
      "756000\n",
      "757000\n",
      "758000\n",
      "759000\n",
      "760000\n",
      "761000\n",
      "762000\n",
      "763000\n",
      "764000\n",
      "765000\n",
      "766000\n",
      "767000\n",
      "768000\n",
      "769000\n",
      "770000\n",
      "771000\n",
      "772000\n",
      "773000\n",
      "774000\n",
      "775000\n",
      "776000\n",
      "777000\n",
      "778000\n",
      "779000\n",
      "780000\n",
      "781000\n",
      "782000\n",
      "783000\n",
      "784000\n",
      "785000\n",
      "786000\n",
      "787000\n",
      "788000\n",
      "789000\n",
      "790000\n",
      "791000\n",
      "792000\n",
      "793000\n",
      "794000\n",
      "795000\n",
      "796000\n",
      "797000\n",
      "798000\n",
      "799000\n",
      "800000\n",
      "801000\n",
      "802000\n",
      "803000\n",
      "804000\n",
      "805000\n",
      "806000\n",
      "807000\n",
      "808000\n",
      "809000\n",
      "810000\n",
      "811000\n",
      "812000\n",
      "813000\n",
      "814000\n",
      "815000\n",
      "816000\n",
      "817000\n",
      "818000\n",
      "819000\n",
      "820000\n",
      "821000\n",
      "822000\n",
      "823000\n",
      "824000\n",
      "825000\n",
      "826000\n",
      "827000\n",
      "828000\n",
      "829000\n",
      "830000\n",
      "831000\n",
      "832000\n",
      "833000\n",
      "834000\n",
      "835000\n",
      "836000\n",
      "837000\n",
      "838000\n",
      "839000\n",
      "840000\n",
      "841000\n",
      "842000\n",
      "843000\n",
      "844000\n",
      "845000\n",
      "846000\n",
      "847000\n",
      "848000\n",
      "849000\n",
      "850000\n",
      "851000\n",
      "852000\n",
      "853000\n",
      "854000\n",
      "855000\n",
      "856000\n",
      "857000\n",
      "858000\n",
      "859000\n",
      "860000\n",
      "861000\n",
      "862000\n",
      "863000\n",
      "864000\n",
      "865000\n",
      "866000\n",
      "867000\n",
      "868000\n",
      "869000\n",
      "870000\n",
      "871000\n",
      "872000\n",
      "873000\n",
      "874000\n",
      "875000\n",
      "876000\n",
      "877000\n",
      "878000\n",
      "879000\n",
      "880000\n",
      "881000\n",
      "882000\n",
      "883000\n",
      "884000\n",
      "885000\n",
      "886000\n",
      "887000\n",
      "888000\n",
      "889000\n",
      "890000\n",
      "891000\n",
      "892000\n",
      "893000\n",
      "894000\n",
      "895000\n",
      "896000\n",
      "897000\n",
      "898000\n",
      "899000\n",
      "900000\n",
      "901000\n",
      "902000\n",
      "903000\n",
      "904000\n",
      "905000\n",
      "906000\n",
      "907000\n",
      "908000\n",
      "909000\n",
      "910000\n",
      "911000\n",
      "912000\n",
      "913000\n",
      "914000\n",
      "915000\n",
      "916000\n",
      "917000\n",
      "918000\n",
      "919000\n",
      "920000\n",
      "921000\n",
      "922000\n",
      "923000\n",
      "924000\n",
      "925000\n",
      "926000\n",
      "927000\n",
      "928000\n",
      "929000\n",
      "930000\n",
      "931000\n",
      "932000\n",
      "933000\n",
      "934000\n",
      "935000\n",
      "936000\n",
      "937000\n",
      "938000\n",
      "939000\n",
      "940000\n",
      "941000\n",
      "942000\n",
      "943000\n",
      "944000\n",
      "945000\n",
      "946000\n",
      "947000\n",
      "948000\n",
      "949000\n",
      "950000\n",
      "951000\n",
      "952000\n",
      "953000\n",
      "954000\n",
      "955000\n",
      "956000\n",
      "957000\n",
      "958000\n",
      "959000\n",
      "960000\n",
      "961000\n",
      "962000\n",
      "963000\n",
      "964000\n",
      "965000\n",
      "966000\n",
      "967000\n",
      "968000\n",
      "969000\n",
      "970000\n",
      "971000\n",
      "972000\n",
      "973000\n",
      "974000\n",
      "975000\n",
      "976000\n",
      "977000\n",
      "978000\n",
      "979000\n",
      "980000\n",
      "981000\n",
      "982000\n",
      "983000\n",
      "984000\n",
      "985000\n",
      "986000\n",
      "987000\n",
      "988000\n",
      "989000\n",
      "990000\n",
      "991000\n",
      "992000\n",
      "993000\n",
      "994000\n",
      "995000\n",
      "996000\n",
      "997000\n",
      "998000\n",
      "999000\n",
      "1000000\n",
      "1001000\n",
      "1002000\n",
      "1003000\n",
      "1004000\n",
      "1005000\n",
      "1006000\n",
      "1007000\n",
      "1008000\n",
      "1009000\n",
      "1010000\n",
      "1011000\n",
      "1012000\n",
      "1013000\n",
      "1014000\n",
      "1015000\n",
      "1016000\n",
      "1017000\n",
      "1018000\n",
      "1019000\n",
      "1020000\n",
      "1021000\n",
      "1022000\n",
      "1023000\n",
      "1024000\n",
      "1025000\n",
      "1026000\n",
      "1027000\n",
      "1028000\n",
      "1029000\n",
      "1030000\n",
      "1031000\n",
      "1032000\n",
      "1033000\n",
      "1034000\n",
      "1035000\n",
      "1036000\n",
      "1037000\n",
      "1038000\n",
      "1039000\n",
      "1040000\n",
      "1041000\n",
      "1042000\n",
      "1043000\n",
      "1044000\n",
      "1045000\n",
      "1046000\n",
      "1047000\n",
      "1048000\n",
      "1049000\n",
      "1050000\n",
      "1051000\n",
      "1052000\n",
      "1053000\n",
      "1054000\n",
      "1055000\n",
      "1056000\n",
      "1057000\n",
      "1058000\n",
      "1059000\n",
      "1060000\n",
      "1061000\n",
      "1062000\n",
      "1063000\n",
      "1064000\n",
      "1065000\n",
      "1066000\n",
      "1067000\n",
      "1068000\n",
      "1069000\n",
      "1070000\n",
      "1071000\n",
      "1072000\n",
      "1073000\n",
      "1074000\n",
      "1075000\n",
      "1076000\n",
      "1077000\n",
      "1078000\n",
      "1079000\n",
      "1080000\n",
      "1081000\n",
      "1082000\n",
      "1083000\n",
      "1084000\n",
      "1085000\n",
      "1086000\n",
      "1087000\n",
      "1088000\n",
      "1089000\n",
      "1090000\n",
      "1091000\n",
      "1092000\n",
      "1093000\n",
      "1094000\n",
      "1095000\n",
      "1096000\n",
      "1097000\n",
      "1098000\n",
      "1099000\n",
      "1100000\n",
      "1101000\n",
      "1102000\n",
      "1103000\n",
      "1104000\n",
      "1105000\n",
      "1106000\n",
      "1107000\n",
      "1108000\n",
      "1109000\n",
      "1110000\n",
      "1111000\n",
      "1112000\n",
      "1113000\n",
      "1114000\n",
      "1115000\n",
      "1116000\n",
      "1117000\n",
      "1118000\n",
      "1119000\n",
      "1120000\n",
      "1121000\n",
      "1122000\n",
      "1123000\n",
      "1124000\n",
      "1125000\n",
      "1126000\n",
      "1127000\n",
      "1128000\n",
      "1129000\n",
      "1130000\n",
      "1131000\n",
      "1132000\n",
      "1133000\n",
      "1134000\n",
      "1135000\n",
      "1136000\n",
      "1137000\n",
      "1138000\n",
      "1139000\n",
      "1140000\n",
      "1141000\n",
      "1142000\n",
      "1143000\n",
      "1144000\n",
      "1145000\n",
      "1146000\n",
      "1147000\n",
      "1148000\n",
      "1149000\n",
      "1150000\n",
      "1151000\n",
      "1152000\n",
      "1153000\n",
      "1154000\n",
      "1155000\n",
      "1156000\n",
      "1157000\n",
      "1158000\n",
      "1159000\n",
      "1160000\n",
      "1161000\n",
      "1162000\n",
      "1163000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164000\n",
      "1165000\n",
      "1166000\n",
      "1167000\n",
      "1168000\n",
      "1169000\n",
      "1170000\n",
      "1171000\n",
      "1172000\n",
      "1173000\n",
      "1174000\n",
      "1175000\n",
      "1176000\n",
      "1177000\n",
      "1178000\n",
      "1179000\n",
      "1180000\n",
      "1181000\n",
      "1182000\n",
      "1183000\n",
      "1184000\n",
      "1185000\n",
      "1186000\n",
      "1187000\n",
      "1188000\n",
      "1189000\n",
      "1190000\n",
      "1191000\n",
      "1192000\n",
      "1193000\n",
      "1194000\n",
      "1195000\n",
      "1196000\n",
      "1197000\n",
      "1198000\n",
      "1199000\n",
      "1200000\n",
      "1201000\n",
      "1202000\n",
      "1203000\n",
      "1204000\n",
      "1205000\n",
      "1206000\n",
      "1207000\n",
      "1208000\n",
      "1209000\n",
      "1210000\n",
      "1211000\n",
      "1212000\n",
      "1213000\n",
      "1214000\n",
      "1215000\n",
      "1216000\n",
      "1217000\n",
      "1218000\n",
      "1219000\n",
      "1220000\n",
      "1221000\n",
      "1222000\n",
      "1223000\n",
      "1224000\n",
      "1225000\n",
      "1226000\n",
      "1227000\n",
      "1228000\n",
      "1229000\n",
      "1230000\n",
      "1231000\n",
      "1232000\n",
      "1233000\n",
      "1234000\n",
      "1235000\n",
      "1236000\n",
      "1237000\n",
      "1238000\n",
      "1239000\n",
      "1240000\n",
      "1241000\n",
      "1242000\n",
      "1243000\n",
      "1244000\n",
      "1245000\n",
      "1246000\n",
      "1247000\n",
      "1248000\n",
      "1249000\n",
      "1250000\n",
      "1251000\n",
      "1252000\n",
      "1253000\n",
      "1254000\n",
      "1255000\n",
      "1256000\n",
      "1257000\n",
      "1258000\n",
      "1259000\n",
      "1260000\n",
      "1261000\n",
      "1262000\n",
      "1263000\n",
      "1264000\n",
      "1265000\n",
      "1266000\n",
      "1267000\n",
      "1268000\n",
      "1269000\n",
      "1270000\n",
      "1271000\n",
      "1272000\n",
      "1273000\n",
      "1274000\n",
      "1275000\n",
      "1276000\n",
      "1277000\n",
      "1278000\n",
      "1279000\n",
      "1280000\n",
      "1281000\n",
      "1282000\n",
      "1283000\n",
      "1284000\n",
      "1285000\n",
      "1286000\n",
      "1287000\n",
      "1288000\n",
      "1289000\n",
      "1290000\n",
      "1291000\n",
      "1292000\n",
      "1293000\n",
      "1294000\n",
      "1295000\n",
      "1296000\n",
      "1297000\n",
      "1298000\n",
      "1299000\n",
      "1300000\n",
      "1301000\n",
      "1302000\n",
      "1303000\n",
      "1304000\n",
      "1305000\n",
      "1306000\n",
      "1307000\n",
      "1308000\n",
      "1309000\n",
      "1310000\n",
      "1311000\n",
      "1312000\n",
      "1313000\n",
      "1314000\n",
      "1315000\n",
      "1316000\n",
      "1317000\n",
      "1318000\n",
      "1319000\n",
      "1320000\n",
      "1321000\n",
      "1322000\n",
      "1323000\n",
      "1324000\n",
      "1325000\n",
      "1326000\n",
      "1327000\n",
      "1328000\n",
      "1329000\n",
      "1330000\n",
      "1331000\n",
      "1332000\n",
      "1333000\n",
      "1334000\n",
      "1335000\n",
      "1336000\n",
      "1337000\n",
      "1338000\n",
      "1339000\n",
      "1340000\n",
      "1341000\n",
      "1342000\n",
      "1343000\n",
      "1344000\n",
      "1345000\n",
      "1346000\n",
      "1347000\n",
      "1348000\n",
      "1349000\n",
      "1350000\n",
      "1351000\n",
      "1352000\n",
      "1353000\n",
      "1354000\n",
      "1355000\n",
      "1356000\n",
      "1357000\n",
      "1358000\n",
      "1359000\n",
      "1360000\n",
      "1361000\n",
      "1362000\n",
      "1363000\n",
      "1364000\n",
      "1365000\n",
      "1366000\n",
      "1367000\n",
      "1368000\n",
      "1369000\n",
      "1370000\n",
      "1371000\n",
      "1372000\n",
      "1373000\n",
      "1374000\n",
      "1375000\n",
      "1376000\n",
      "1377000\n",
      "1378000\n",
      "1379000\n",
      "1380000\n",
      "1381000\n",
      "1382000\n",
      "1383000\n",
      "1384000\n",
      "1385000\n",
      "1386000\n",
      "1387000\n",
      "1388000\n",
      "1389000\n",
      "1390000\n",
      "1391000\n",
      "1392000\n",
      "1393000\n",
      "1394000\n",
      "1395000\n",
      "1396000\n",
      "1397000\n",
      "1398000\n",
      "1399000\n",
      "1400000\n",
      "1401000\n",
      "1402000\n",
      "1403000\n",
      "1404000\n",
      "1405000\n",
      "1406000\n",
      "1407000\n",
      "1408000\n",
      "1409000\n",
      "1410000\n",
      "1411000\n",
      "1412000\n",
      "1413000\n",
      "1414000\n",
      "1415000\n",
      "1416000\n",
      "1417000\n",
      "1418000\n",
      "1419000\n",
      "1420000\n",
      "1421000\n",
      "1422000\n",
      "1423000\n",
      "1424000\n",
      "1425000\n",
      "1426000\n",
      "1427000\n",
      "1428000\n",
      "1429000\n",
      "1430000\n",
      "1431000\n",
      "1432000\n",
      "1433000\n",
      "1434000\n",
      "1435000\n",
      "1436000\n",
      "1437000\n",
      "1438000\n",
      "1439000\n",
      "1440000\n",
      "1441000\n",
      "1442000\n",
      "1443000\n",
      "1444000\n",
      "1445000\n",
      "1446000\n",
      "1447000\n",
      "1448000\n",
      "1449000\n",
      "1450000\n",
      "1451000\n",
      "1452000\n",
      "1453000\n",
      "1454000\n",
      "1455000\n",
      "1456000\n",
      "1457000\n",
      "1458000\n",
      "1459000\n",
      "1460000\n",
      "1461000\n",
      "1462000\n",
      "1463000\n",
      "1464000\n",
      "1465000\n",
      "1466000\n",
      "1467000\n",
      "1468000\n",
      "1469000\n",
      "1470000\n",
      "1471000\n",
      "1472000\n",
      "1473000\n",
      "1474000\n",
      "1475000\n",
      "1476000\n",
      "1477000\n",
      "1478000\n",
      "1479000\n",
      "1480000\n",
      "1481000\n",
      "1482000\n",
      "1483000\n",
      "1484000\n",
      "1485000\n",
      "1486000\n",
      "1487000\n",
      "1488000\n",
      "1489000\n",
      "1490000\n",
      "1491000\n",
      "1492000\n",
      "1493000\n",
      "1494000\n",
      "1495000\n",
      "1496000\n",
      "1497000\n",
      "1498000\n",
      "1499000\n",
      "1500000\n",
      "1501000\n",
      "1502000\n",
      "1503000\n",
      "1504000\n",
      "1505000\n",
      "1506000\n",
      "1507000\n",
      "1508000\n",
      "1509000\n",
      "1510000\n",
      "1511000\n",
      "1512000\n",
      "1513000\n",
      "1514000\n",
      "1515000\n",
      "1516000\n",
      "1517000\n",
      "1518000\n",
      "1519000\n",
      "1520000\n",
      "1521000\n",
      "1522000\n",
      "1523000\n",
      "1524000\n",
      "1525000\n",
      "1526000\n",
      "1527000\n",
      "1528000\n",
      "1529000\n",
      "1530000\n",
      "1531000\n",
      "1532000\n",
      "1533000\n",
      "1534000\n",
      "1535000\n",
      "1536000\n",
      "1537000\n",
      "1538000\n",
      "1539000\n",
      "1540000\n",
      "1541000\n",
      "1542000\n",
      "1543000\n",
      "1544000\n",
      "1545000\n",
      "1546000\n",
      "1547000\n",
      "1548000\n",
      "1549000\n",
      "1550000\n",
      "1551000\n",
      "1552000\n",
      "1553000\n",
      "1554000\n",
      "1555000\n",
      "1556000\n",
      "1557000\n",
      "1558000\n",
      "1559000\n",
      "1560000\n",
      "1561000\n",
      "1562000\n",
      "1563000\n",
      "1564000\n",
      "1565000\n",
      "1566000\n",
      "1567000\n",
      "1568000\n",
      "1569000\n",
      "1570000\n",
      "1571000\n",
      "1572000\n",
      "1573000\n",
      "1574000\n",
      "1575000\n",
      "1576000\n",
      "1577000\n",
      "1578000\n",
      "1579000\n",
      "1580000\n",
      "1581000\n",
      "1582000\n",
      "1583000\n",
      "1584000\n",
      "1585000\n",
      "1586000\n",
      "1587000\n",
      "1588000\n",
      "1589000\n",
      "1590000\n",
      "1591000\n",
      "1592000\n",
      "1593000\n",
      "1594000\n",
      "1595000\n",
      "1596000\n",
      "1597000\n",
      "1598000\n",
      "1599000\n",
      "1600000\n",
      "1601000\n",
      "1602000\n",
      "1603000\n",
      "1604000\n",
      "1605000\n",
      "1606000\n",
      "1607000\n",
      "1608000\n",
      "1609000\n",
      "1610000\n",
      "1611000\n",
      "1612000\n",
      "1613000\n",
      "1614000\n",
      "1615000\n",
      "1616000\n",
      "1617000\n",
      "1618000\n",
      "1619000\n",
      "1620000\n",
      "1621000\n",
      "1622000\n",
      "1623000\n",
      "1624000\n",
      "1625000\n",
      "1626000\n",
      "1627000\n",
      "1628000\n",
      "1629000\n",
      "1630000\n",
      "1631000\n",
      "1632000\n",
      "1633000\n",
      "1634000\n",
      "1635000\n",
      "1636000\n",
      "1637000\n",
      "1638000\n",
      "1639000\n",
      "1640000\n",
      "1641000\n",
      "1642000\n",
      "1643000\n",
      "1644000\n",
      "1645000\n",
      "1646000\n",
      "1647000\n",
      "1648000\n",
      "1649000\n",
      "1650000\n",
      "1651000\n",
      "1652000\n",
      "1653000\n",
      "1654000\n",
      "1655000\n",
      "1656000\n",
      "1657000\n",
      "1658000\n",
      "1659000\n",
      "1660000\n",
      "1661000\n",
      "1662000\n",
      "1663000\n",
      "1664000\n",
      "1665000\n",
      "1666000\n",
      "1667000\n",
      "1668000\n",
      "1669000\n",
      "1670000\n",
      "1671000\n",
      "1672000\n",
      "1673000\n",
      "1674000\n",
      "1675000\n",
      "1676000\n",
      "1677000\n",
      "1678000\n",
      "1679000\n",
      "1680000\n",
      "1681000\n",
      "1682000\n",
      "1683000\n",
      "1684000\n",
      "1685000\n",
      "1686000\n",
      "1687000\n",
      "1688000\n",
      "1689000\n",
      "1690000\n",
      "1691000\n",
      "1692000\n",
      "1693000\n",
      "1694000\n",
      "1695000\n",
      "1696000\n",
      "1697000\n",
      "1698000\n",
      "1699000\n",
      "1700000\n",
      "1701000\n",
      "1702000\n",
      "1703000\n",
      "1704000\n",
      "1705000\n",
      "1706000\n",
      "1707000\n",
      "1708000\n",
      "1709000\n",
      "1710000\n",
      "1711000\n",
      "1712000\n",
      "1713000\n",
      "1714000\n",
      "1715000\n",
      "1716000\n",
      "1717000\n",
      "1718000\n",
      "1719000\n",
      "1720000\n",
      "1721000\n",
      "1722000\n",
      "1723000\n",
      "1724000\n",
      "1725000\n",
      "1726000\n",
      "1727000\n",
      "1728000\n",
      "1729000\n",
      "1730000\n",
      "1731000\n",
      "1732000\n",
      "1733000\n",
      "1734000\n",
      "1735000\n",
      "1736000\n",
      "1737000\n",
      "1738000\n",
      "1739000\n",
      "1740000\n",
      "1741000\n",
      "1742000\n",
      "1743000\n",
      "1744000\n",
      "1745000\n",
      "1746000\n",
      "1747000\n",
      "1748000\n",
      "1749000\n",
      "1750000\n",
      "1751000\n",
      "1752000\n",
      "1753000\n",
      "1754000\n",
      "1755000\n",
      "1756000\n",
      "1757000\n",
      "1758000\n",
      "1759000\n",
      "1760000\n",
      "1761000\n",
      "1762000\n",
      "1763000\n",
      "1764000\n",
      "1765000\n",
      "1766000\n",
      "1767000\n",
      "1768000\n",
      "1769000\n",
      "1770000\n",
      "1771000\n",
      "1772000\n",
      "1773000\n",
      "1774000\n",
      "1775000\n",
      "1776000\n",
      "1777000\n",
      "1778000\n",
      "1779000\n",
      "1780000\n",
      "1781000\n",
      "1782000\n",
      "1783000\n",
      "1784000\n",
      "1785000\n",
      "1786000\n",
      "1787000\n",
      "1788000\n",
      "1789000\n",
      "1790000\n",
      "1791000\n",
      "1792000\n",
      "1793000\n",
      "1794000\n",
      "1795000\n",
      "1796000\n",
      "1797000\n",
      "1798000\n",
      "1799000\n",
      "1800000\n",
      "1801000\n",
      "1802000\n",
      "1803000\n",
      "1804000\n",
      "1805000\n",
      "1806000\n",
      "1807000\n",
      "1808000\n",
      "1809000\n",
      "1810000\n",
      "1811000\n",
      "1812000\n",
      "1813000\n",
      "1814000\n",
      "1815000\n",
      "1816000\n",
      "1817000\n",
      "1818000\n",
      "1819000\n",
      "1820000\n",
      "1821000\n",
      "1822000\n",
      "1823000\n",
      "1824000\n",
      "1825000\n",
      "1826000\n",
      "1827000\n",
      "1828000\n",
      "1829000\n",
      "1830000\n",
      "1831000\n",
      "1832000\n",
      "1833000\n",
      "1834000\n",
      "1835000\n",
      "1836000\n",
      "1837000\n",
      "1838000\n",
      "1839000\n",
      "1840000\n",
      "1841000\n",
      "1842000\n",
      "1843000\n",
      "1844000\n",
      "1845000\n",
      "1846000\n",
      "1847000\n",
      "1848000\n",
      "1849000\n",
      "1850000\n",
      "1851000\n",
      "1852000\n",
      "1853000\n",
      "1854000\n",
      "1855000\n",
      "1856000\n",
      "1857000\n",
      "1858000\n",
      "1859000\n",
      "1860000\n",
      "1861000\n",
      "1862000\n",
      "1863000\n",
      "1864000\n",
      "1865000\n",
      "1866000\n",
      "1867000\n",
      "1868000\n",
      "1869000\n",
      "1870000\n",
      "1871000\n",
      "1872000\n",
      "1873000\n",
      "1874000\n",
      "1875000\n",
      "1876000\n",
      "1877000\n",
      "1878000\n",
      "1879000\n",
      "1880000\n",
      "1881000\n",
      "1882000\n",
      "1883000\n",
      "1884000\n",
      "1885000\n",
      "1886000\n",
      "1887000\n",
      "1888000\n",
      "1889000\n",
      "1890000\n",
      "1891000\n",
      "1892000\n",
      "1893000\n",
      "1894000\n",
      "1895000\n",
      "1896000\n",
      "1897000\n",
      "1898000\n",
      "1899000\n",
      "1900000\n",
      "1901000\n",
      "1902000\n",
      "1903000\n",
      "1904000\n",
      "1905000\n",
      "1906000\n",
      "1907000\n",
      "1908000\n",
      "1909000\n",
      "1910000\n",
      "1911000\n",
      "1912000\n",
      "1913000\n",
      "1914000\n",
      "1915000\n",
      "1916000\n",
      "1917000\n",
      "1918000\n",
      "1919000\n",
      "1920000\n",
      "1921000\n",
      "1922000\n",
      "1923000\n",
      "1924000\n",
      "1925000\n",
      "1926000\n",
      "1927000\n",
      "1928000\n",
      "1929000\n",
      "1930000\n",
      "1931000\n",
      "1932000\n",
      "1933000\n",
      "1934000\n",
      "1935000\n",
      "1936000\n",
      "1937000\n",
      "1938000\n",
      "1939000\n",
      "1940000\n",
      "1941000\n",
      "1942000\n",
      "1943000\n",
      "1944000\n",
      "1945000\n",
      "1946000\n",
      "1947000\n",
      "1948000\n",
      "1949000\n",
      "1950000\n",
      "1951000\n",
      "1952000\n",
      "1953000\n",
      "1954000\n",
      "1955000\n",
      "1956000\n",
      "1957000\n",
      "1958000\n",
      "1959000\n",
      "1960000\n",
      "1961000\n",
      "1962000\n",
      "1963000\n",
      "1964000\n",
      "1965000\n",
      "1966000\n",
      "1967000\n",
      "1968000\n",
      "1969000\n",
      "1970000\n",
      "1971000\n",
      "1972000\n",
      "1973000\n",
      "1974000\n",
      "1975000\n",
      "1976000\n",
      "1977000\n",
      "1978000\n",
      "1979000\n",
      "1980000\n",
      "1981000\n",
      "1982000\n",
      "1983000\n",
      "1984000\n",
      "1985000\n",
      "1986000\n",
      "1987000\n",
      "1988000\n",
      "1989000\n",
      "1990000\n",
      "1991000\n",
      "1992000\n",
      "1993000\n",
      "1994000\n",
      "1995000\n",
      "1996000\n",
      "1997000\n",
      "1998000\n"
     ]
    }
   ],
   "source": [
    "tailleDB = 1998403\n",
    "picTot = 0\n",
    "\n",
    "DB = np.zeros([tailleDB, 16, 16, 4])\n",
    "classY = np.zeros(tailleDB, dtype=int)\n",
    "\n",
    "countlbl = np.zeros([23,2])\n",
    "for i in range(22):\n",
    "    countlbl[i+1][0] = countlbl[i][0] + 1\n",
    "\n",
    "batchLen = 1000\n",
    "\n",
    "while picTot < tailleDB:\n",
    "    ra = np.random.randint(0,18698240)\n",
    "    dataBatch = f['S2'][ra:ra+batchLen]\n",
    "    classBatch = f['TOP_LANDCOVER'][ra:ra+batchLen]\n",
    "    pic = 0\n",
    "    while pic < batchLen:\n",
    "        if (countlbl[int(classBatch[pic]),1] < lblComplete[int(classBatch[pic]),1]) and (picTot < tailleDB):\n",
    "            DB[picTot] = dataBatch[pic]\n",
    "            classY[picTot] = classBatch[pic]\n",
    "            picTot +=1\n",
    "            countlbl[int(classBatch[pic]),1] +=1\n",
    "            if picTot%1000==0:\n",
    "                print(picTot)\n",
    "        pic += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.fft as ft\n",
    "i=0\n",
    "DB[i][:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3400\n",
      "6800\n",
      "10200\n",
      "13600\n",
      "17000\n",
      "20400\n",
      "23800\n",
      "27200\n",
      "30600\n",
      "34000\n",
      "37400\n",
      "40800\n",
      "44200\n",
      "47600\n",
      "51000\n",
      "54400\n",
      "57800\n",
      "61200\n",
      "64600\n",
      "68000\n",
      "71400\n",
      "74800\n",
      "78200\n",
      "81600\n",
      "85000\n",
      "88400\n",
      "91800\n",
      "95200\n",
      "98600\n",
      "102000\n",
      "105400\n",
      "108800\n",
      "112200\n",
      "115600\n",
      "119000\n",
      "122400\n",
      "125800\n",
      "129200\n",
      "132600\n",
      "136000\n",
      "139400\n",
      "142800\n",
      "146200\n",
      "149600\n",
      "153000\n",
      "156400\n",
      "159800\n",
      "163200\n",
      "166600\n",
      "170000\n",
      "173400\n",
      "176800\n",
      "180200\n",
      "183600\n",
      "187000\n",
      "190400\n",
      "193800\n",
      "197200\n",
      "200600\n",
      "204000\n",
      "207400\n",
      "210800\n",
      "214200\n",
      "217600\n",
      "221000\n",
      "224400\n",
      "227800\n",
      "231200\n",
      "234600\n",
      "238000\n",
      "241400\n",
      "244800\n",
      "248200\n",
      "251600\n",
      "255000\n",
      "258400\n",
      "261800\n",
      "265200\n",
      "268600\n",
      "272000\n",
      "275400\n",
      "278800\n",
      "282200\n",
      "285600\n",
      "289000\n",
      "292400\n",
      "295800\n",
      "299200\n",
      "302600\n",
      "306000\n",
      "309400\n",
      "312800\n",
      "316200\n",
      "319600\n",
      "323000\n",
      "326400\n",
      "329800\n",
      "333200\n",
      "336600\n",
      "340000\n",
      "343400\n",
      "346800\n",
      "350200\n",
      "353600\n",
      "357000\n",
      "360400\n",
      "363800\n",
      "367200\n",
      "370600\n",
      "374000\n",
      "377400\n",
      "380800\n",
      "384200\n",
      "387600\n",
      "391000\n",
      "394400\n",
      "397800\n",
      "401200\n",
      "404600\n",
      "408000\n",
      "411400\n",
      "414800\n",
      "418200\n",
      "421600\n",
      "425000\n",
      "428400\n",
      "431800\n",
      "435200\n",
      "438600\n",
      "442000\n",
      "445400\n",
      "448800\n",
      "452200\n",
      "455600\n",
      "459000\n",
      "462400\n",
      "465800\n",
      "469200\n",
      "472600\n",
      "476000\n",
      "479400\n",
      "482800\n",
      "486200\n",
      "489600\n",
      "493000\n",
      "496400\n",
      "499800\n",
      "503200\n",
      "506600\n",
      "510000\n",
      "513400\n",
      "516800\n",
      "520200\n",
      "523600\n",
      "527000\n",
      "530400\n",
      "533800\n",
      "537200\n",
      "540600\n",
      "544000\n",
      "547400\n",
      "550800\n",
      "554200\n",
      "557600\n",
      "561000\n",
      "564400\n",
      "567800\n",
      "571200\n",
      "574600\n",
      "578000\n",
      "581400\n",
      "584800\n",
      "588200\n",
      "591600\n",
      "595000\n",
      "598400\n",
      "601800\n",
      "605200\n",
      "608600\n",
      "612000\n",
      "615400\n",
      "618800\n",
      "622200\n",
      "625600\n",
      "629000\n",
      "632400\n",
      "635800\n",
      "639200\n",
      "642600\n",
      "646000\n",
      "649400\n",
      "652800\n",
      "656200\n",
      "659600\n",
      "663000\n",
      "666400\n",
      "669800\n",
      "673200\n",
      "676600\n",
      "680000\n",
      "683400\n",
      "686800\n",
      "690200\n",
      "693600\n",
      "697000\n"
     ]
    }
   ],
   "source": [
    "fourrier = np.zeros((tailleDB,16,16,4))\n",
    "for i in range(tailleDB):\n",
    "    if i%3400==0:\n",
    "        print(i)\n",
    "    for canal in range((4)):\n",
    "        fourrier[i][:,:,canal]=np.absolute(ft.fft2(DB[i][:,:,canal]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourrier[12].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(16,16,4)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "model2.add(Conv2D(32, (5, 5), input_shape=input_shape))\n",
    "\n",
    "model2.add(Conv2D(32, (5, 5)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(32, (4, 4)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.01))\n",
    "model2.add(Dense(23))\n",
    "model2.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model2.compile(optimizer=optim,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(h5_path, batch_size, coucou):\n",
    "    f = h5.File(h5_path, 'r')\n",
    "    while True : \n",
    "        idxs = coucou\n",
    "        batch_count = get_batch_count(idxs, batch_size)\n",
    "        for b in range(batch_count):\n",
    "            batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "            batch_idxs = sorted(batch_idxs)\n",
    "            X = fourrier[batch_idxs, :,:,:]\n",
    "            Y = classY[batch_idxs]\n",
    "            yield np.array(X), keras.utils.np_utils.to_categorical(np.array(Y), 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idxs_BD = range(tailleDB)\n",
    "idxs_test = range(taille_test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_idxs = shuffle_idx(idxs)\n",
    "train_idxs, val_idxs = split_train_val(shuffled_idxs, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(PATH_DATA, BATCH_SIZE, train_idxs)\n",
    "train_batch_count = get_batch_count(train_idxs, BATCH_SIZE)\n",
    "\n",
    "val_gen = generator(PATH_DATA, BATCH_SIZE, val_idxs)\n",
    "val_batch_count = get_batch_count(val_idxs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559550"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  19/1000 [..............................] - ETA: 9s - loss: 1.3220 - acc: 0.5362"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=10, validation_steps=100, validation_data=<generator..., steps_per_epoch=1000, verbose=1)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3209 - acc: 0.5315 - val_loss: 1.2992 - val_acc: 0.5341\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3235 - acc: 0.5266 - val_loss: 1.2740 - val_acc: 0.5487\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3076 - acc: 0.5343 - val_loss: 1.2404 - val_acc: 0.5559\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3117 - acc: 0.5307 - val_loss: 1.2402 - val_acc: 0.5581\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3111 - acc: 0.5292 - val_loss: 1.2650 - val_acc: 0.5513\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3164 - acc: 0.5292 - val_loss: 1.2529 - val_acc: 0.5472\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.3067 - acc: 0.5316 - val_loss: 1.2218 - val_acc: 0.5684\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.2972 - acc: 0.5377 - val_loss: 1.2606 - val_acc: 0.5525\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.2963 - acc: 0.5363 - val_loss: 1.2192 - val_acc: 0.5778\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 1.2938 - acc: 0.5353 - val_loss: 1.2541 - val_acc: 0.5534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fefa15b8dd8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(train_gen,steps_per_epoch=1000, epochs=10, verbose=1, validation_data=val_gen, nb_val_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = h5.File('data/pred_eighties_from_full_1_without_gt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"S2\": shape (241700, 16, 16, 4), type \"<f4\">"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['S2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "taille_test_db = len(test['S2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3400\n",
      "6800\n",
      "10200\n",
      "13600\n",
      "17000\n",
      "20400\n",
      "23800\n",
      "27200\n",
      "30600\n",
      "34000\n",
      "37400\n",
      "40800\n",
      "44200\n",
      "47600\n",
      "51000\n",
      "54400\n",
      "57800\n",
      "61200\n",
      "64600\n",
      "68000\n",
      "71400\n",
      "74800\n",
      "78200\n",
      "81600\n",
      "85000\n",
      "88400\n",
      "91800\n",
      "95200\n",
      "98600\n",
      "102000\n",
      "105400\n",
      "108800\n",
      "112200\n",
      "115600\n",
      "119000\n",
      "122400\n",
      "125800\n",
      "129200\n",
      "132600\n",
      "136000\n",
      "139400\n",
      "142800\n",
      "146200\n",
      "149600\n",
      "153000\n",
      "156400\n",
      "159800\n",
      "163200\n",
      "166600\n",
      "170000\n",
      "173400\n",
      "176800\n",
      "180200\n",
      "183600\n",
      "187000\n",
      "190400\n",
      "193800\n",
      "197200\n",
      "200600\n",
      "204000\n",
      "207400\n",
      "210800\n",
      "214200\n",
      "217600\n",
      "221000\n",
      "224400\n",
      "227800\n",
      "231200\n",
      "234600\n",
      "238000\n",
      "241400\n"
     ]
    }
   ],
   "source": [
    "fourrier_test = np.zeros((taille_test_db,16,16,4))\n",
    "for i in range(taille_test_db):\n",
    "    if i%3400==0:\n",
    "        print(i)\n",
    "    for canal in range((4)):\n",
    "        fourrier[i][:,:,canal]=np.absolute(ft.fft2(DB[i][:,:,canal]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_generator(batch_size, idxs):\n",
    "    \n",
    "\n",
    "    batch_count = get_batch_count(idxs, batch_size)\n",
    "    \n",
    "    for b in range(batch_count):\n",
    "        batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "        batch_idxs = sorted(batch_idxs)\n",
    "        X = fourrier_test[batch_idxs, :,:,:]\n",
    "        yield np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gen = prediction_generator(BATCH_SIZE,idxs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7554/7554 [==============================] - 20s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "resultat = model2.predict_generator(pred_gen,steps=get_batch_count(idxs_test, BATCH_SIZE), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prediction = np.argmax(resultat, axis = 1)\n",
    "tosubmit = pd.DataFrame([idxs_test,class_prediction]).transpose()\n",
    "tosubmit.columns=[\"ID\",\"TOP_LANDCOVER\"]\n",
    "to_submit_csv = tosubmit.to_csv('FFT_2.csv',sep=',',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241700"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tosubmit.head()\n",
    "len(tosubmit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(16,16,4)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "model3.add(Conv2D(32, (5, 5), input_shape=input_shape))\n",
    "\n",
    "model3.add(Conv2D(32, (5, 5)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "model3.add(Conv2D(32, (4, 4)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(64))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.01))\n",
    "model3.add(Dense(23))\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(h5_path, batch_size, coucou):\n",
    "    f = h5.File(h5_path, 'r')\n",
    "    while True : \n",
    "        idxs = coucou\n",
    "        batch_count = get_batch_count(idxs, batch_size)\n",
    "        for b in range(batch_count):\n",
    "            batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "            batch_idxs = sorted(batch_idxs)\n",
    "            X = DB[batch_idxs, :,:,:]\n",
    "            Y = classY[batch_idxs]\n",
    "            yield np.array(X), keras.utils.np_utils.to_categorical(np.array(Y), 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_idxs = shuffle_idx(idxs)\n",
    "train_idxs, val_idxs = split_train_val(shuffled_idxs, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(PATH_DATA, BATCH_SIZE, train_idxs)\n",
    "train_batch_count = get_batch_count(train_idxs, BATCH_SIZE)\n",
    "\n",
    "val_gen = generator(PATH_DATA, BATCH_SIZE, val_idxs)\n",
    "val_batch_count = get_batch_count(val_idxs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model3.compile(optimizer=optim,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=100, validation_steps=100, validation_data=<generator..., steps_per_epoch=1000, verbose=1)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2355503 is out of bounds for axis 0 with size 1998403",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-67305d26243b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-9308d94b2077>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(h5_path, batch_size, coucou)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mbatch_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbatch_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2355503 is out of bounds for axis 0 with size 1998403"
     ]
    }
   ],
   "source": [
    "model3.fit_generator(train_gen,steps_per_epoch=1000, epochs=100, verbose=1, validation_data=val_gen, nb_val_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_generator(batch_size, idxs):\n",
    "    \n",
    "\n",
    "    batch_count = get_batch_count(idxs, batch_size)\n",
    "    \n",
    "    for b in range(batch_count):\n",
    "        batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "        batch_idxs = sorted(batch_idxs)\n",
    "        X = test['S2'][batch_idxs, :,:,:]\n",
    "        yield np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gen = prediction_generator(BATCH_SIZE,idxs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7554/7554 [==============================] - 24s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "resultat = model3.predict_generator(pred_gen,steps=get_batch_count(idxs_test, BATCH_SIZE), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prediction = np.argmax(resultat, axis = 1)\n",
    "tosubmit = pd.DataFrame([idxs_test,class_prediction]).transpose()\n",
    "tosubmit.columns=[\"ID\",\"TOP_LANDCOVER\"]\n",
    "to_submit_csv = tosubmit.to_csv('model3_v1.csv',sep=',',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241700, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5f = h5py.File('Db_proportions.h5', 'w')\n",
    "h5f.create_dataset('dataset_prop', data=DB)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB[0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = h5py.File('Db_proportions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in verif.items():\n",
    "    print(element[0])\n",
    "    print(element[1])\n",
    "    print(element[1].name)\n",
    "verif.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_elmts = [key for key in verif['/'].keys()]\n",
    "for key in list_elmts:\n",
    "    print(key)\n",
    "    print(type(verif['/'][key]))\n",
    "    print(verif['/'][key])\n",
    "    print([key for key in verif['/'][key].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs_test = get_idxs(PATH_PREDICT_WITHOUT_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_batch_count, val_batch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1\n",
    "input_shape = (16,16,4)\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "model.add(Conv2D(8,(5,5),activation='relu',input_shape =(16,16,4)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(8,(5,5),activation='relu',input_shape =(16,16,4)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200,activation ='relu'))\n",
    "model.add(Dropout(0.01))\n",
    "\n",
    "model.add(Dense(23,activation ='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2\n",
    "input_shape=(16,16,4)\n",
    "model2 = Sequential()\n",
    "model2.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "model2.add(Conv2D(32, (5, 5), input_shape=input_shape))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(32, (5, 5)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(32, (4, 4)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.01))\n",
    "model2.add(Dense(23))\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim = keras.optimizers.Adam(lr=0.001)\n",
    "optim = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model2.compile(optimizer=optim,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model2.fit_generator(train_gen, steps_per_epoch=100, epochs=4, verbose=1, validation_data=val_gen, nb_val_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i, (name, values) in enumerate(history.history.items()):\n",
    "    plt.subplot(1, len(history.history.items()), i+1)\n",
    "    plt.plot(values)\n",
    "    plt.title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction routines\n",
    "\n",
    "In order to submit a result here are some gits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def prediction_generator(h5_path, batch_size, idxs):\n",
    "    f = h5.File(h5_path, 'r')\n",
    "\n",
    "    batch_count = get_batch_count(idxs, batch_size)\n",
    "    \n",
    "    for b in range(batch_count):\n",
    "        batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "        batch_idxs = sorted(batch_idxs)\n",
    "        X = f['S2'][batch_idxs, :,:,:]\n",
    "        yield np.array(X)\n",
    "\n",
    "def build_h5_pred_file(pred, h5_output_path):\n",
    "    if os.path.exists(h5_output_path):\n",
    "        os.remove(h5_output_path)\n",
    "    f = h5.File(h5_output_path, 'w')\n",
    "    top_landcover_submit = f.create_dataset(\"TOP_LANDCOVER\", (len(pred), 1), maxshape=(None, 1))\n",
    "    top_landcover_submit[:, 0] = pred\n",
    "    f.close()\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx = get_idxs(PATH_PREDICT_WITHOUT_GT)\n",
    "print(len(pred_idx))\n",
    "pred_gen = prediction_generator(PATH_PREDICT_WITHOUT_GT, BATCH_SIZE, pred_idx)\n",
    "prediction = model2.predict_generator(pred_gen, steps=get_batch_count(pred_idx, BATCH_SIZE), verbose=1)\n",
    "print(prediction)\n",
    "#build_h5_pred_file(np.argmax(prediction, axis = 1), PATH_SUBMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultat_avec_csv(modele,name,path):\n",
    "\n",
    "    pred_idx = get_idxs(path)\n",
    "    pred_gen = prediction_generator(path, BATCH_SIZE, pred_idx)\n",
    "    prediction = modele.predict_generator(pred_gen, steps=get_batch_count(pred_idx, BATCH_SIZE), verbose=1)\n",
    "    class_prediction = np.argmax(prediction, axis = 1)\n",
    "    tosubmit = pd.DataFrame([pred_idx,class_prediction]).transpose()\n",
    "    tosubmit.columns=[\"ID\",\"TOP_LANDCOVER\"]\n",
    "    to_submit_csv = tosubmit.to_csv('%s.csv'%(name),sep=',',index= False)\n",
    "    return prediction\n",
    "\n",
    "def resultat(modele,path):\n",
    "\n",
    "    pred_idx = get_idxs(path)\n",
    "    pred_gen = prediction_generator(path, BATCH_SIZE, pred_idx)\n",
    "    prediction = modele.predict_generator(pred_gen, steps=get_batch_count(pred_idx, BATCH_SIZE), verbose=1)\n",
    "\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai =resultat(model2,PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat_avec_csv(model2,\"jubois_palmi\",PATH_PREDICT_WITHOUT_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some ideas for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7700/32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_generator(h5_path, batch_size, idxs):\n",
    "    f = h5.File(h5_path, 'r')\n",
    "\n",
    "    batch_count = get_batch_count(idxs, batch_size)\n",
    "    print(batch_count)\n",
    "    for b in range(batch_count):\n",
    "        if (b+1)*batch_size<\n",
    "        batch_idxs = idxs[b*batch_size:(b+1)*batch_size]\n",
    "        batch_idxs = sorted(batch_idxs)\n",
    "        print(max(batch_idxs))\n",
    "        Y = f['TOP_LANDCOVER'][batch_idxs, :]\n",
    "        yield keras.utils.np_utils.to_categorical(np.array(Y), 23)\n",
    "\n",
    "gt_gen = gt_generator(PATH_DATA, BATCH_SIZE, pred_idx)\n",
    "gt = []\n",
    "for elem in gt_gen:\n",
    "    gt.append(elem)\n",
    "gt = np.vstack(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' #if normalize else '.i'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\",fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_confusion_matrix(confusion_matrix, classes):\n",
    "    real_classes = []\n",
    "    for c in range(len(classes)):\n",
    "        if np.sum(confusion_matrix[:,c])+np.sum(confusion_matrix[c, :]) != 0:\n",
    "            real_classes.append(c)\n",
    "    real_confusion_matrix = np.empty((len(real_classes), len(real_classes)))  \n",
    "    for c_index in range(len(real_classes)):\n",
    "        real_confusion_matrix[c_index,:] = confusion_matrix[real_classes[c_index], real_classes]\n",
    "    return real_confusion_matrix, real_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_top=list(f['TOP_LANDCOVER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = np.array(list_top)\n",
    "y_pred = np.argmax(essai, axis = 1)\n",
    "\n",
    "real_cnf_matrix, real_classes = clean_confusion_matrix(confusion_matrix(y_true, y_pred, labels= range(23)), range(23))\n",
    "plot_confusion_matrix(real_cnf_matrix, classes = real_classes, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_top[:20][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
